# Basic benchmark configuration for PyTorch transformer benchmarks
# Usage: python score_mod.py --config config_basic.yaml

# Core parameters
dynamic: false
calculate_bwd: true
dtype: "bfloat16"

# Shape parameters - larger sweep
b: [1]  # batch sizes
nh: ["32,32"]  # [query_heads,key_value_heads]
s: [512]  # sequence lengths
d: [64]  # head dimensions (limited to 128 for Flash Attention/cuDNN compatibility)

# All attention types
mods: ["noop", "causal"]

# Multiple backends for comparison (SDPA + Flash Attention) - flex is always included internally
backend: ["fav2"]
max_autotune: true  # Enable torch.compile with max-autotune for optimal performance

# Decoding and cache settings
decoding: false
kv_size: null

# Metrics and output
throughput: true  # Calculate memory bandwidth & TFLOPS
save_path: "comprehensive_results.csv"  # Save to CSV
output_json_for_dashboard: "attn_bench_basic.json"
